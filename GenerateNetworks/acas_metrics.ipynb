{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efea92d9-d92f-4d60-84b1-01dbb33276dc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a1b16-33b4-4e9d-a4f8-07436149b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa5bf0-c8ff-47cb-aebc-7e69f6272e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import h5py\n",
    "from keras.optimizers import Adamax, Nadam\n",
    "import sys\n",
    "from writeNNet import saveNNet\n",
    "\n",
    "from interval import interval, inf\n",
    "\n",
    "from safe_train import propagate_interval, check_intervals, plot_policy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4262f-a64a-4513-95d2-e21c5324c727",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e30f12-45b6-4dec-a580-ffb614208ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = 4  # Neural network version\n",
    "hu = 45  # Number of hidden units in each hidden layer in network\n",
    "saveEvery = 3  # Epoch frequency of saving\n",
    "totalEpochs = 20  # Total number of training epochs\n",
    "BATCH_SIZE = 2**8\n",
    "EPOCH_TO_PROJECT = 5\n",
    "trainingDataFiles = (\n",
    "    \"../TrainingData/VertCAS_TrainingData_v2_%02d.h5\"  # File format for training data\n",
    ")\n",
    "nnetFiles = (\n",
    "    \"../networks/SafeVertCAS_pra%02d_v%d_45HU_%03d.nnet\"  # File format for .nnet files\n",
    ")\n",
    "advisories = {\n",
    "    \"COC\": 0,\n",
    "    \"DNC\": 1,\n",
    "    \"DND\": 2,\n",
    "    \"DES1500\": 3,\n",
    "    \"CL1500\": 4,\n",
    "    \"SDES1500\": 5,\n",
    "    \"SCL1500\": 6,\n",
    "    \"SDES2500\": 7,\n",
    "    \"SCL2500\": 8,\n",
    "}\n",
    "\n",
    "pra = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35270cd-40d6-41c9-bbef-b79ed4a4cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Data for VertCAS, pra %02d, Network Version %d\" % (pra, ver))\n",
    "f = h5py.File(trainingDataFiles % pra, \"r\")\n",
    "X_train = np.array(f[\"X\"])\n",
    "Q = np.array(f[\"y\"])\n",
    "means = np.array(f[\"means\"])\n",
    "ranges = np.array(f[\"ranges\"])\n",
    "min_inputs = np.array(f[\"min_inputs\"])\n",
    "max_inputs = np.array(f[\"max_inputs\"])\n",
    "print(f\"min inputs: {min_inputs}\")\n",
    "print(f\"max inputs: {max_inputs}\")\n",
    "\n",
    "N, numOut = Q.shape\n",
    "print(f\"Setting up model with {numOut} outputs and {N} training examples\")\n",
    "num_batches = N / BATCH_SIZE\n",
    "\n",
    "# Asymmetric loss function\n",
    "lossFactor = 40.0\n",
    "\n",
    "# NOTE(nskh): from HorizontalCAS which was updated to use TF\n",
    "def asymMSE(y_true, y_pred):\n",
    "    d = y_true - y_pred\n",
    "    maxes = tf.argmax(y_true, axis=1)\n",
    "    maxes_onehot = tf.one_hot(maxes, numOut)\n",
    "    others_onehot = maxes_onehot - 1\n",
    "    d_opt = d * maxes_onehot\n",
    "    d_sub = d * others_onehot\n",
    "    a = lossFactor * (numOut - 1) * (tf.square(d_opt) + tf.abs(d_opt))\n",
    "    b = tf.square(d_opt)\n",
    "    c = lossFactor * (tf.square(d_sub) + tf.abs(d_sub))\n",
    "    d = tf.square(d_sub)\n",
    "    loss = tf.where(d_sub > 0, c, d) + tf.where(d_opt > 0, a, b)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2573b-4065-4d70-b1ce-1a8962901cf9",
   "metadata": {},
   "source": [
    "# Training: Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb30bf-1160-40b0-8915-c1fffdffcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpochs = 20\n",
    "saveEvery = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c97b9-8eb5-47a8-af28-109c1e7aafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "# model.add(Dense(hu, init='uniform', activation='relu', input_dim=4))\n",
    "# model.add(Dense(hu, init='uniform', activation='relu'))\n",
    "# model.add(Dense(hu, init='uniform', activation='relu'))\n",
    "# model.add(Dense(hu, init='uniform', activation='relu'))\n",
    "# model.add(Dense(hu, init='uniform', activation='relu'))\n",
    "# model.add(Dense(hu, init='uniform', activation='relu'))\n",
    "model.add(Dense(hu, activation=\"relu\", input_dim=4))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "\n",
    "# model.add(Dense(numOut, init=\"uniform\"))\n",
    "model.add(Dense(numOut))\n",
    "opt = Nadam(learning_rate=0.0003)\n",
    "model.compile(loss=asymMSE, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# # Train and write nnet files\n",
    "epoch = saveEvery\n",
    "while epoch <= totalEpochs:\n",
    "    model.fit(X_train, Q, epochs=saveEvery, batch_size=2**8, shuffle=True)\n",
    "    saveFile = nnetFiles % (pra, ver, epoch)\n",
    "    saveNNet(model, saveFile, means, ranges, min_inputs, max_inputs)\n",
    "    epoch += saveEvery\n",
    "    output_interval, penultimate_interval = propagate_interval(\n",
    "        [\n",
    "            interval[400, 500],\n",
    "            interval[50, 51],\n",
    "            interval[-51, -50],\n",
    "            interval[20, 21],\n",
    "        ],\n",
    "        model,\n",
    "        graph=False,\n",
    "    )\n",
    "    print(output_interval)\n",
    "    plot_policy(model, f\"images/standard_vcas_policy_viz_vo50_vi-50_epoch{epoch}.pdf\", zoom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c6768-a1ee-459c-a4f9-0155a0d93adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/july6-standard-acas-8epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5d1dc-b5d3-49b9-b0a6-0eaf184ad2e4",
   "metadata": {},
   "source": [
    "# Querying standard model for estimates of loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b6109-672b-41c8-97e9-4ffe99e271c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COC_INTERVAL = [\n",
    "    interval[400, 500],\n",
    "    interval[50, 51],\n",
    "    interval[-51, -50],\n",
    "    interval[20, 21],\n",
    "]\n",
    "action_names = [\n",
    "    \"COC\",\n",
    "    \"DNC\",\n",
    "    \"DND\",\n",
    "    \"DES1500\",\n",
    "    \"CL1500\",\n",
    "    \"SDES1500\",\n",
    "    \"SCL1500\",\n",
    "    \"SDES2500\",\n",
    "    \"SCL2500\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bcb9c-8c44-4e74-98ce-f53703f9d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.meshgrid(*[np.arange(400, 510, 25), np.arange(50, 51.1, 0.25), np.arange(-51, -49.9, 0.25)[::-1], np.arange(20, 21.1, 0.25)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4affb5-6631-434d-bdae-56504ed94fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.vstack([np.arange(400, 510, 25), np.arange(50, 51.1, 0.25), np.arange(-51, -49.9, 0.25)[::-1], np.arange(20, 21.1, 0.25)]).T\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671290ba-b1d0-43a6-b74c-95b52acc83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c53f09-566d-48f7-9de1-98da0e4b875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "advisory_idxs = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437a89c-ee1d-4696-8b0d-45d969a6a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[action_names[idx] for idx in advisory_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8f1e2-6963-4613-99d5-0500a915a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.array([[400, 50, -50, 20]]))\n",
    "action_names[np.argmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6af4b9-abab-4e3f-a4a5-804805b2e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce682e5c-7fc9-4f68-a4c6-0ced3f264298",
   "metadata": {},
   "source": [
    "# Plotting Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b83ab0-f31f-4587-b4b3-b6251cb8a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3c24c-3c0c-4347-ae66-a86a8e0bc823",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4d0f4-282b-435c-bb8c-ac9ea575a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = np.hstack([np.linspace(-5000, -2000, 20), np.linspace(-2000, 2000, 40), np.linspace(2000, 5000, 20)])\n",
    "hs.shape\n",
    "\n",
    "vo = 50\n",
    "vi = -50\n",
    "x_grid = None\n",
    "taus = np.linspace(0, 40, 81)\n",
    "for tau in taus:\n",
    "    grid_component = np.vstack([hs, np.ones(hs.shape)*vo, np.ones(hs.shape) * vi, np.ones(hs.shape)*tau]).T\n",
    "    if x_grid is not None:\n",
    "        x_grid = np.vstack([x_grid, grid_component])\n",
    "    else:\n",
    "        x_grid = grid_component\n",
    "    \n",
    "x_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587360b5-2387-4abf-acda-72db85523c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_grid[:, 3], x_grid[:, 0], s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45712f2a-a4e6-493d-9b85-a93e08670044",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_grid)\n",
    "advisory_idxs = np.argmax(y_pred, axis=1)\n",
    "commands = [action_names[idx] for idx in advisory_idxs]\n",
    "\n",
    "ra1 = (0.9,0.9,0.9) # white\n",
    "ra2 = (.0,1.0,1.0) # cyan\n",
    "ra3 = (144.0/255.0,238.0/255.0,144.0/255.0) # lightgreen\n",
    "ra4 = (30.0/255.0,144.0/255.0,1.0) # dodgerblue\n",
    "ra5 = (0.0,1.0,.0) # lime\n",
    "ra6 = (0.0,0.0,1.0) # blue\n",
    "ra7 = (34.0/255.0,139.0/255.0,34.0/255.0) # forestgreen\n",
    "ra8 = (0.0,0.0,128.0/255.0) # navy\n",
    "ra9 = (0.0,100.0/255.0,0.0) # darkgreen\n",
    "colors = [ra1,ra2,ra3,ra4,ra5,ra6,ra7,ra8,ra9]\n",
    "bg_colors = [(1.0,1.0,1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387c9d5-4718-4497-90c1-2548f3440d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict indexed by color/advisory of all points\n",
    "xs = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n",
    "ys = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n",
    "\n",
    "for i, advisory_idx in enumerate(advisory_idxs):\n",
    "    color = colors[advisory_idx]\n",
    "    scatter_x = x_grid[i, 3] # tau\n",
    "    scatter_y = x_grid[i, 0] # h \n",
    "    xs[advisory_idx].append(scatter_x)\n",
    "    ys[advisory_idx].append(scatter_y)\n",
    "print(\"done constructing dicts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7931df-f479-4392-bf2b-9a7d9da45849",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.tight_layout()\n",
    "for i in range(len(colors)):\n",
    "    plt.scatter(xs[i], ys[i], s = 10, c = [colors[i]])\n",
    "plt.legend(action_names)\n",
    "plt.xlabel(\"Tau (sec)\")\n",
    "plt.ylabel(\"h (ft)\")\n",
    "plt.title(f\"Policy for vo:{vo} and vi:{vi}\")\n",
    "plt.savefig(f\"viz_policy_vo{vo}_vi{vi}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044901db-cd36-46cb-8fde-cf85c03822e4",
   "metadata": {},
   "source": [
    "# Training: \"safe\" with projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630b82c-e069-47b7-9829-9f6c2fa56917",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([1, 2, 3.4]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc575e8-7312-4ef5-9c84-2f9e16fa1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(hu, activation=\"relu\", input_dim=4))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "\n",
    "# model.add(Dense(numOut, init=\"uniform\"))\n",
    "model.add(Dense(numOut))\n",
    "opt = Nadam(learning_rate=0.0003)\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "weights_before_projection = []\n",
    "weights_after_projection = []\n",
    "for epoch in range(totalEpochs):\n",
    "    print(f\"on epoch {epoch}\")\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    train_indices = np.arange(X_train.shape[0])\n",
    "\n",
    "    rng.shuffle(train_indices)  # in-place\n",
    "\n",
    "    x_shuffled = X_train[train_indices, :]\n",
    "    y_shuffled = Q[train_indices, :]\n",
    "\n",
    "    x_batched = np.split(\n",
    "        x_shuffled, np.arange(BATCH_SIZE, len(x_shuffled), BATCH_SIZE)\n",
    "    )\n",
    "    y_batched = np.split(\n",
    "        y_shuffled, np.arange(BATCH_SIZE, len(y_shuffled), BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset_batched = list(zip(x_batched, y_batched))\n",
    "    batch_losses = []\n",
    "    epoch_accuracy = keras.metrics.CategoricalAccuracy()\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(dataset_batched):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss = asymMSE(y_batch_train, y_pred)\n",
    "            batch_losses.append(loss.numpy())\n",
    "            epoch_accuracy.update_state(y_batch_train, y_pred)\n",
    "\n",
    "        if step % int(num_batches / 500) == 0:\n",
    "            print(\n",
    "                f\"{np.round(step / num_batches * 100, 1)}% through this epoch with loss\",\n",
    "                f\"{loss.numpy()} and accuracy {epoch_accuracy.result()}\\r\",\n",
    "                end=\"\",\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        opt.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "    epoch_accuracies.append(epoch_accuracy.result())\n",
    "    epoch_losses.append(np.mean(batch_losses))\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Mean loss over this epoch: {np.mean(batch_losses)}\")\n",
    "    print(f\"Mean accuracy over this epoch: {epoch_accuracy.result()}\")\n",
    "    \n",
    "    weights_before_projection.append(model.layers[-1].weights)\n",
    "    \n",
    "    # Parameters:\n",
    "    # - h (ft): Altitude of intruder relative to ownship, [-8000, 8000]\n",
    "    # - vO (ft/s): ownship vertical climb rate, [-100, 100]\n",
    "    # - vI (ft/s): intruder vertical climb rate, [-100, 100]\n",
    "    # - τ (sec): time to loss of horizontal separation\n",
    "    output_interval, penultimate_interval = propagate_interval(\n",
    "        [\n",
    "            interval[7880, 7900],\n",
    "            interval[95, 96],\n",
    "            interval[5, 6],\n",
    "            interval[38, 40],\n",
    "        ],\n",
    "        model,\n",
    "        graph=False,\n",
    "    )\n",
    "    \n",
    "    if not check_intervals(output_interval, desired_interval):\n",
    "        print(f\"safe region test FAILED, interval was {output_interval}\")\n",
    "        if epoch % EPOCH_TO_PROJECT == 0:\n",
    "            print(f\"\\nProjecting weights at epoch {epoch}.\")\n",
    "            intervals_to_project = []\n",
    "            assert type(output_interval) == type(desired_interval)\n",
    "            if type(output_interval) is list:\n",
    "                assert len(output_interval) == len(desired_interval)\n",
    "                for i in range(len(output_interval)):\n",
    "                    if (\n",
    "                        desired_interval[i] is not None\n",
    "                        and output_interval[i] not in desired_interval[i]\n",
    "                    ):\n",
    "                        intervals_to_project.append(i)\n",
    "            else:\n",
    "                intervals_to_project.append(0)\n",
    "\n",
    "            weights_tf = model.layers[-1].weights\n",
    "            weights_np = weights_tf[0].numpy()\n",
    "            biases_np = weights_tf[1].numpy()\n",
    "\n",
    "            for idx in intervals_to_project:\n",
    "                weights_to_project = np.hstack([weights_np[:, idx], biases_np[idx]])\n",
    "                proj = project_weights(\n",
    "                    desired_interval[idx], penultimate_interval, weights_to_project\n",
    "                )\n",
    "                weights_np[:, idx] = proj[:-1]\n",
    "                biases_np[idx] = proj[-1]\n",
    "\n",
    "            model.layers[-1].set_weights([weights_np, biases_np])\n",
    "            output_interval, _ = propagate_interval(\n",
    "                COC_INTERVAL,\n",
    "                model,\n",
    "                graph=False,\n",
    "            )\n",
    "            weights_after_projection.append(model.layers[-1].weights)\n",
    "            print(f\"After projecting, output interval is {output_interval}\")\n",
    "    else:\n",
    "        print(f\"safe region test passed, interval was {output_interval}\")\n",
    "        \n",
    "    with open(\"projection_acas.pickle\", \"wb\") as f:\n",
    "        data = {\n",
    "            \"accuracies\": epoch_accuracies,\n",
    "            \"losses\": epoch_losses,\n",
    "            \"weights_before_projection\": weights_before_projection,\n",
    "            \"weights_after_projection\" : weights_after_projection\n",
    "        }\n",
    "        pickle.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dc815-1478-4f9f-8224-6cf77672da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(hu, activation=\"relu\", input_dim=4))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "\n",
    "# model.add(Dense(numOut, init=\"uniform\"))\n",
    "model.add(Dense(numOut))\n",
    "opt = Nadam(learning_rate=0.0003)\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "train_indices = np.arange(X_train.shape[0])\n",
    "\n",
    "rng.shuffle(train_indices)  # in-place\n",
    "\n",
    "x_shuffled = X_train[train_indices, :]\n",
    "y_shuffled = Q[train_indices, :]\n",
    "\n",
    "x_batched = np.split(\n",
    "    x_shuffled, np.arange(BATCH_SIZE, len(x_shuffled), BATCH_SIZE)\n",
    ")\n",
    "y_batched = np.split(\n",
    "    y_shuffled, np.arange(BATCH_SIZE, len(y_shuffled), BATCH_SIZE)\n",
    ")\n",
    "\n",
    "dataset_batched = list(zip(x_batched, y_batched))\n",
    "batch_losses = []\n",
    "epoch_accuracy = keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b467e0-ce66-41db-94f9-c54fb32568df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch_train, y_batch_train = dataset_batched[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfae0f-fe25-40eb-9256-daefe6c4cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cffa2a-e294-4148-b839-21f3b7a4914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43243dad-0e37-4a86-8aee-99388d7f9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d934f2b-05fd-46c6-beff-35c4e7ced974",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf9563-37c1-4153-b93a-f2524f06510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y_pred = model(x_batch_train, training=True)  # Forward pass\n",
    "    loss = asymMSE(y_batch_train, y_pred)\n",
    "    batch_losses.append(loss.numpy())\n",
    "    epoch_accuracy.update_state(y_batch_train, y_pred)\n",
    "    print(epoch_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79e522-f784-48d5-b112-616485a2527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be6e5a-0fbb-4764-8021-37dad8ddaa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9896f9-607e-41ef-86cb-744aa7065aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf6048-feb8-4a3b-bf2f-50ae74200ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(y_batch_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee89c21-936b-4281-8968-bb3fe08b3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmin(y_pred, axis=1) == np.argmin(y_batch_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a72d4c-56c6-4be2-9d9a-08c6cb145c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
