{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7777b62-b94e-4ad5-8d26-9cc280e19cd4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdaf31b-2887-4d89-94cf-2f14cb20c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d26d3-ea86-4ad6-a94d-32183dbb143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import h5py\n",
    "from keras.optimizers import Adamax, Nadam\n",
    "import sys\n",
    "import pickle\n",
    "from writeNNet import saveNNet\n",
    "\n",
    "from interval import interval, inf\n",
    "\n",
    "from maraboupy import Marabou, MarabouCore\n",
    "\n",
    "from safe_train import propagate_interval, check_intervals, project_weights, plot_policy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a02409-44ae-4bf1-9ca0-3b6cef4cb1e4",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d85caa-1831-4e0c-b53a-c5bd5d4e14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = 4  # Neural network version\n",
    "hu = 45  # Number of hidden units in each hidden layer in network\n",
    "totalEpochs = 20  # Total number of training epochs\n",
    "BATCH_SIZE = 2**8\n",
    "EPOCH_TO_PROJECT = 1\n",
    "trainingDataFiles = (\n",
    "    \"../TrainingData/VertCAS_TrainingData_v2_%02d.h5\"  # File format for training data\n",
    ")\n",
    "nnetFiles = \"../networks/ProjectionVertCAS_pra%02d_v%d_45HU_%03d.nnet\"  # File format for .nnet files\n",
    "COC_INTERVAL = [\n",
    "    interval[400, 500],\n",
    "    interval[50, 51],\n",
    "    interval[-51, -50],\n",
    "    interval[20, 21],\n",
    "]\n",
    "# COC high, SDES2500 low\n",
    "desired_interval = [\n",
    "    interval[7000, 15000],  # COC\n",
    "    None,  # DNC\n",
    "    None,  # DND\n",
    "    None,  # DES1500\n",
    "    None,  # CL1500\n",
    "    None,  # SDES1500\n",
    "    None,  # SCL1500\n",
    "    interval[-2000, 6000],  # SDES2500\n",
    "    None,  # SCL2500\n",
    "]\n",
    "advisories = {\n",
    "    \"COC\": 0,\n",
    "    \"DNC\": 1,\n",
    "    \"DND\": 2,\n",
    "    \"DES1500\": 3,\n",
    "    \"CL1500\": 4,\n",
    "    \"SDES1500\": 5,\n",
    "    \"SCL1500\": 6,\n",
    "    \"SDES2500\": 7,\n",
    "    \"SCL2500\": 8,\n",
    "}\n",
    "pra = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c40fa9-5b6c-4dda-934c-3d0c7b8f103a",
   "metadata": {},
   "source": [
    "# Constants for training, setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ae9aa-d478-4d87-9f73-48b5c7226519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Data for VertCAS, pra %02d, Network Version %d\" % (pra, ver))\n",
    "f = h5py.File(trainingDataFiles % pra, \"r\")\n",
    "X_train = np.array(f[\"X\"])\n",
    "Q = np.array(f[\"y\"])\n",
    "means = np.array(f[\"means\"])\n",
    "ranges = np.array(f[\"ranges\"])\n",
    "min_inputs = np.array(f[\"min_inputs\"])\n",
    "max_inputs = np.array(f[\"max_inputs\"])\n",
    "print(f\"min inputs: {min_inputs}\")\n",
    "print(f\"max inputs: {max_inputs}\")\n",
    "\n",
    "N, numOut = Q.shape\n",
    "print(f\"Setting up model with {numOut} outputs and {N} training examples\")\n",
    "num_batches = N / BATCH_SIZE\n",
    "\n",
    "\n",
    "# Asymmetric loss function\n",
    "lossFactor = 40.0\n",
    "\n",
    "# NOTE(nskh): from HorizontalCAS which was updated to use TF\n",
    "def asymMSE(y_true, y_pred):\n",
    "    d = y_true - y_pred\n",
    "    maxes = tf.argmax(y_true, axis=1)\n",
    "    maxes_onehot = tf.one_hot(maxes, numOut)\n",
    "    others_onehot = maxes_onehot - 1\n",
    "    d_opt = d * maxes_onehot\n",
    "    d_sub = d * others_onehot\n",
    "    a = lossFactor * (numOut - 1) * (tf.square(d_opt) + tf.abs(d_opt))\n",
    "    b = tf.square(d_opt)\n",
    "    c = lossFactor * (tf.square(d_sub) + tf.abs(d_sub))\n",
    "    d = tf.square(d_sub)\n",
    "    loss = tf.where(d_sub > 0, c, d) + tf.where(d_opt > 0, a, b)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb9d0e-502b-4e59-8fcd-d0f79dc63f1d",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763b4bc-ff81-4b9b-bf4f-1641a943748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(hu, activation=\"relu\", input_dim=4))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "model.add(Dense(hu, activation=\"relu\"))\n",
    "\n",
    "# model.add(Dense(numOut, init=\"uniform\"))\n",
    "model.add(Dense(numOut))\n",
    "opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.0003)\n",
    "model.compile(loss=asymMSE, optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711367a4-fd47-4d9e-bcc8-878aa1ef84c1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d4a0e-3886-42d2-8c6d-fbd02ac05c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_safe_weights = None\n",
    "last_safe_epoch = 0\n",
    "num_unsafe_epochs = 0\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "weights_before_projection = []\n",
    "weights_after_projection = []\n",
    "for epoch in range(totalEpochs):\n",
    "    print(f\"on epoch {epoch}\")\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    train_indices = np.arange(X_train.shape[0])\n",
    "\n",
    "    rng.shuffle(train_indices)  # in-place\n",
    "\n",
    "    x_shuffled = X_train[train_indices, :]\n",
    "    y_shuffled = Q[train_indices, :]\n",
    "\n",
    "    x_batched = np.split(\n",
    "        x_shuffled, np.arange(BATCH_SIZE, len(x_shuffled), BATCH_SIZE)\n",
    "    )\n",
    "    y_batched = np.split(\n",
    "        y_shuffled, np.arange(BATCH_SIZE, len(y_shuffled), BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    dataset_batched = list(zip(x_batched, y_batched))\n",
    "    batch_losses = []\n",
    "    batch_accuracy_list = []\n",
    "    epoch_accuracy = keras.metrics.CategoricalAccuracy()\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(dataset_batched):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss = asymMSE(y_batch_train, y_pred)\n",
    "            epoch_accuracy.update_state(y_batch_train, y_pred)\n",
    "\n",
    "            # accumulate data\n",
    "            batch_losses.append(loss.numpy())\n",
    "            batch_accuracy_list.append(epoch_accuracy.result())\n",
    "        if step % int(num_batches / 500) == 0:\n",
    "            print(\n",
    "                f\"{np.round(step / num_batches * 100, 1)}% through this epoch with loss\",\n",
    "                f\"{np.round(loss.numpy(), 5)} and accuracy {np.round(epoch_accuracy.result(), 5)}\\r\",\n",
    "                end=\"\",\n",
    "            )\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        opt.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    epoch_accuracies.append(batch_accuracy_list)\n",
    "    epoch_losses.append(batch_losses)\n",
    "\n",
    "    weights_before_projection.append([w.numpy() for w in model.layers[-1].weights])\n",
    "    print(\"Policy Viz\")\n",
    "    plot_policy(model, f\"exploration_viz_vo50_vi-50_epoch{epoch}.pdf\", vo=50, vi=-50)\n",
    "    if epoch == 0:\n",
    "        last_safe_weights = model.get_weights()\n",
    "    # TODO here downward for Marabou integration\n",
    "\n",
    "    # Parameters:\n",
    "    # - h (ft): Altitude of intruder relative to ownship, [-8000, 8000]\n",
    "    # - vO (ft/s): ownship vertical climb rate, [-100, 100]\n",
    "    # - vI (ft/s): intruder vertical climb rate, [-100, 100]\n",
    "    # - Ï„ (sec): time to loss of horizontal separation\n",
    "    output_interval, penultimate_interval = propagate_interval(\n",
    "        COC_INTERVAL,\n",
    "        model,\n",
    "        graph=False,\n",
    "    )\n",
    "\n",
    "    print(\"With Marabou:\\n\")\n",
    "    tf.saved_model.save(model, \"tmp\")\n",
    "    network = Marabou.read_tf(\"tmp\", modelType=\"savedModel_v2\")\n",
    "\n",
    "    inputVars = network.inputVars[0][0]\n",
    "    outputVars = network.outputVars[0][0]\n",
    "\n",
    "    print(\"input constraints\")\n",
    "    for i, in_int in enumerate(COC_INTERVAL):\n",
    "        print(inputVars[i], \">\", in_int[0].inf)\n",
    "        network.setLowerBound(inputVars[i], in_int[0].inf)\n",
    "        print(inputVars[i], \"<\", in_int[0].sup)\n",
    "        network.setUpperBound(inputVars[i], in_int[0].sup)\n",
    "\n",
    "    print(\"output constraints\")\n",
    "    for i, des_int in enumerate(desired_interval):\n",
    "        if des_int is None:\n",
    "            continue\n",
    "        print(outputVars[i], \">\", des_int[0].inf)\n",
    "        print(outputVars[i], \"<\", des_int[0].sup)\n",
    "\n",
    "        ineq1 = MarabouCore.Equation(MarabouCore.Equation.LE)\n",
    "        ineq1.addAddend(outputVars[i], 1)\n",
    "        ineq1.setScalar(des_int[0].inf)\n",
    "\n",
    "        ineq2 = MarabouCore.Equation(MarabouCore.Equation.GE)\n",
    "        ineq2.addAddend(outputVars[i], 1)\n",
    "        ineq2.setScalar(des_int[0].sup)\n",
    "        disjunction = [[ineq1], [ineq2]]\n",
    "        network.addDisjunctionConstraint(disjunction)\n",
    "\n",
    "        # Check relative ordering of outputs?\n",
    "        print(\"Add max constraint\")\n",
    "        print(f\"{outputVars[0]} should be max among outputVars\")\n",
    "        the_max_var_idx = 0\n",
    "        for i, var in enumerate(outputVars):\n",
    "            if i == the_max_var_idx:\n",
    "                continue\n",
    "            print(f\"{outputVars[the_max_var_idx]} - {outputVars[i]} > 0\")\n",
    "            network.addInequality([outputVars[the_max_var_idx], outputVars[i]], [1, -1], 0)\n",
    "\n",
    "    _, vals, stats = network.solve(\"marabou.log\")\n",
    "    if vals == {}:\n",
    "        print(\"UNSAT. So safe region test passed.\")\n",
    "        last_safe_weights = model.get_weights()\n",
    "        last_safe_epoch = epoch\n",
    "        num_unsafe_epochs = 0\n",
    "    else:\n",
    "        print(f\"safe region test FAILED, counterexample {vals}\")\n",
    "        print(vals)\n",
    "        num_unsafe_epochs += 1\n",
    "\n",
    "    if num_unsafe_epochs == 10:\n",
    "        print(\"Exploration budget exhausted.\")\n",
    "        print(\"Restarting training from last safe epoch.\")\n",
    "        model.set_weights(last_safe_weights)\n",
    "        num_unsafe_epochs = 0\n",
    "\n",
    "    with open(\"exploration_budget_acas.pickle\", \"wb\") as f:\n",
    "        data = {\n",
    "            \"accuracies\": epoch_accuracies,\n",
    "            \"losses\": epoch_losses,\n",
    "            \"weights_before_projection\": weights_before_projection,\n",
    "        }\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a0184-ef33-4a4f-b8c6-094c32cc0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0732806-e9c1-4a74-9fd4-1ad6019e8eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ea559-7c1f-4172-8577-59da57e8b989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
